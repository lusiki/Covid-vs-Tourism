Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
#head(Crosentilex_sve)
CroSentilex_Gold  <- read.delim2(mfiles_downlaod_txt("0", 136680, 136712, ext = ".txt"),
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" )
CroSentilex_Gold[1,1] <- "dati"
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
#head(CroSentilex_Gold)
# leksikoni
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
my_stop_words <- tibble(
word = c(
"jedan",
"e","prvi", "dva","dvije","drugi",
"tri","treći","pet","kod",
"ove","ova",  "ovo","bez",
"evo","oko",  "om", "ek",
"mil","tko","šest", "sedam",
"osam",   "čim", "zbog",
"prema", "dok","zato", "koji",
"im", "čak","među", "tek",
"koliko", "tko","kod","poput",
"baš", "dakle", "osim", "svih",
"svoju", "odnosno", "gdje",
"kojoj", "ovi", "toga"
),
lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
bind_rows(stopwords_cro)
kandidati %>%
unnest_tokens(word, txtVector) %>%
anti_join(stop_corpus, by = "word") %>%
mutate(word = gsub("\\d+", NA, word)) %>%
mutate(word = gsub("^[a-zA-Z]$", NA, word)) %>%
filter(!is.na(word)) -> rijeci_clean
rijeci_clean %>%
group_by(word) %>%
count() %>%
arrange(desc(n)) %>%
top_n(100) -> pregled
##korpus bigram
kandidati %>%
unnest_tokens(bigram, txtVector, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) -> kandidati_bigram
bigrams_separated <- kandidati_bigram %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_corpus$word) %>%
filter(!word2 %in% stop_corpus$word) %>%
filter(!grepl("\\d+", word1)) %>%
filter(!grepl("\\d+", word2)) %>%
filter(!grepl("^[a-zA-Z]$", word1)) %>%
filter(!grepl("^[a-zA-Z]$", word2))
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
vazni_domena <- rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
#  mutate(Datum = floor_date(datum, "week")) %>%
group_by(domena) %>%
count(word) %>%
mutate(ukupno_n = sum(n)) %>%
arrange(desc(n)) %>% #View()
filter(word %in% c("Mima",
"Lesnina",
"Harvey Norman")) %>% #View()
ggplot(., aes(word, n / ukupno_n , fill = word  )) +
geom_bar(stat = "identity") +
facet_wrap(~ domena, scales = "free_y")   +
scale_y_continuous(labels = scales::percent_format()) +
ggtitle("Zastupljenost \"najizglednijih\" kandidata na portalima") +
ylab("Udio %") +
xlab("") +
labs(caption = "*Udio se odnosi na učestalost pojavljivanja imena kandidata u ukupnom broju riječi na domeni")+
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
vazni_domena
kandidatiLista <- c("Mima namještaj")
kandidatiLista <- enc2utf8(kandidatiLista)
datumOd <- "2019-06-01"
datumDo <- Sys.Date() - 1
# quick search
kandidat_i <- list()
for (i in 1:length(kandidatiLista)) {
kandidatUrlencode <- RCurl::curlEscape(kandidatiLista[i])
x <- mfiles_get(token, paste0("/objects.aspx?q=", kandidatUrlencode, "&p1022>>=", datumOd, "&limit=50000"))[[1]] # "&p1030=vecernji"
kandidat_i[[i]] <- cbind.data.frame(x, kandidat = kandidatiLista[i])
}
kandidatDocs <- do.call(rbind, kandidat_i)
Sys.sleep(1L)
# get metadata
clanciProp <- list()
for (i in 1:length(kandidatDocs$Title)) {
x <- mfiles_get(token, paste0("/objects/0/", kandidatDocs$DisplayID[i], "/latest/properties"))
clanciProp[[i]]  <- t(x[x$PropertyDef %in% c("0", "1020", "1022", "1023", "1021", "1028",
"1027", "1026", "1031", "1024", "1025", "1030"),
c("TypedValue.DisplayValue")])
}
metadata <- do.call(rbind, clanciProp)
metadata <- as.data.frame(metadata, stringsAsFactors = FALSE)
colnames(metadata) <- c("naziv", "id", "naslov", "datum", "vrijeme", "pogledi", "label", "brojKomentara",
"linkKomentari", "autor", "domena", "poveznica")
metadata <- cbind.data.frame(metadata, kandidat = as.character(kandidatDocs$kandidat), stringsAsFactors = FALSE)
# povuci txt fileove
txtVector <- vector("character", nrow(metadata))
for (i in 1:nrow(metadata)) {
txtVector[i] <- mfiles_downlaod("0", kandidatDocs$DisplayID[i], kandidatDocs$Files[[i]]$ID)
}
kandidati <- cbind.data.frame(metadata, txtVector, stringsAsFactors = FALSE)
kandidati$datum <- as.Date(kandidati$datum, "%m/%d/%Y")
kandidati$kandidat <- enc2utf8(kandidati$kandidat)
CroSentilex_n <- read.delim(mfiles_downlaod_txt("0", 136679, 136711, ext = ".txt"),
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "NEG")
CroSentilex_p <- read.delim(mfiles_downlaod_txt("0", 136681, 136713, ext = ".txt"),
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" ) %>%
mutate(brija = "POZ")
Crosentilex_sve <- rbind(setDT(CroSentilex_n), setDT(CroSentilex_p))
#head(Crosentilex_sve)
CroSentilex_Gold  <- read.delim2(mfiles_downlaod_txt("0", 136680, 136712, ext = ".txt"),
header = FALSE,
sep = " ",
stringsAsFactors = FALSE) %>%
rename(word = "V1", sentiment = "V2" )
CroSentilex_Gold[1,1] <- "dati"
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "-", "1")
CroSentilex_Gold$sentiment <- str_replace(CroSentilex_Gold$sentiment , "\\+", "2")
CroSentilex_Gold$sentiment <- as.numeric(unlist(CroSentilex_Gold$sentiment))
#head(CroSentilex_Gold)
# leksikoni
stopwords_cro <- get_stopwords(language = "hr", source = "stopwords-iso")
my_stop_words <- tibble(
word = c(
"jedan",
"e","prvi", "dva","dvije","drugi",
"tri","treći","pet","kod",
"ove","ova",  "ovo","bez",
"evo","oko",  "om", "ek",
"mil","tko","šest", "sedam",
"osam",   "čim", "zbog",
"prema", "dok","zato", "koji",
"im", "čak","među", "tek",
"koliko", "tko","kod","poput",
"baš", "dakle", "osim", "svih",
"svoju", "odnosno", "gdje",
"kojoj", "ovi", "toga"
),
lexicon = "lux"
)
stop_corpus <- my_stop_words %>%
bind_rows(stopwords_cro)
View(kandidati)
library(readxl)
mima <- read.xlsx("C:/Users/Lukas7OneDrive/Desktop")
library(xlsx)
mima <- read.xlsx("C:/Users/Lukas7OneDrive/Desktop")
mima <- read.xlsx("C:/Users/Lukas7OneDrive/Desktop",sheetIndex, header=TRUE)
mima <- read.xlsx("C:/Users/Lukas7OneDrive/Desktop",sheetIndex=1, header=TRUE)
?read.xlsx
mima <- read.xlsx("C:/Users/Lukas7OneDrive/Desktop",1, header=TRUE)
mima <- read.xlsx("C:/Users/Lukas7OneDrive/Desktop/mima.xlsx",1, header=TRUE)
mima <- read.xlsx("C:/Users/Lukas7OneDrive/Desktop/mima.xlsx",1)
mima <- read.csv2("C:/Users/Lukas7OneDrive/Desktop/mima.xlsx")
mima <- read.csv2("C:/Users/Lukas/OneDrive/Desktop/mima.xlsx")
View(mima)
mima <- read.csv2("C:/Users/Lukas/OneDrive/Desktop/mima.csv")
View(mima)
mima %>%
unnest_tokens(word, MENTION_SNIPPET) %>%
anti_join(stop_corpus, by = "word") %>%
mutate(word = gsub("\\d+", NA, word)) %>%
mutate(word = gsub("^[a-zA-Z]$", NA, word)) %>%
filter(!is.na(word)) -> rijeci_clean
rijeci_clean %>%
group_by(word) %>%
count() %>%
arrange(desc(n)) %>%
top_n(100) -> pregled
kandidati %>%
unnest_tokens(bigram, txtVector, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) -> kandidati_bigram
bigrams_separated <- kandidati_bigram %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_corpus$word) %>%
filter(!word2 %in% stop_corpus$word) %>%
filter(!grepl("\\d+", word1)) %>%
filter(!grepl("\\d+", word2)) %>%
filter(!grepl("^[a-zA-Z]$", word1)) %>%
filter(!grepl("^[a-zA-Z]$", word2))
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
mima %>%
unnest_tokens(bigram, txtVector, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) -> kandidati_bigram
mima %>%
unnest_tokens(bigram, MENTION_SNIPPET, token = "ngrams", n = 2) %>%
count(bigram, sort = TRUE) -> kandidati_bigram
bigrams_separated <- kandidati_bigram %>%
separate(bigram, c("word1", "word2"), sep = " ")
bigrams_filtered <- bigrams_separated %>%
filter(!word1 %in% stop_corpus$word) %>%
filter(!word2 %in% stop_corpus$word) %>%
filter(!grepl("\\d+", word1)) %>%
filter(!grepl("\\d+", word2)) %>%
filter(!grepl("^[a-zA-Z]$", word1)) %>%
filter(!grepl("^[a-zA-Z]$", word2))
# new bigram counts:
bigram_counts <- bigrams_filtered %>%
count(word1, word2, sort = TRUE)
bigrams_united <- bigrams_filtered %>%
unite(bigram, word1, word2, sep = " ")
prosjek <- mima %>%
mutate(Datum = floor_date(datum, "week")) %>%
group_by(Datum, FROM) %>%
count() %>%
ungroup() %>%
summarise(nmean = mean(n))
mima$DATE <- as.Date(kandidati$DATE, "%Y-%m-%d")
mima$DATE <- as.Date(mima$DATE, "%Y-%m-%d")
str(mima)
mima$DATE <- as.Date(mima$DATE, "%Y-%m-%d")
prosjek <- mima %>%
mutate(Datum = floor_date(DATE, "week")) %>%
group_by(DATE, FROM) %>%
count() %>%
ungroup() %>%
summarise(nmean = mean(n))
br_cl_dom <- kandidati %>%
mutate(Datum = floor_date(DATE, "week")) %>%
group_by(Datum, FROM) %>%
count() %>%
arrange(desc(FROM)) %>%
ggplot(.,aes(Datum, n)) +
geom_line(size = 1.1) +
facet_wrap(~ FROM) +
theme_bw() +
#  geom_hline(yintercept = 40.5	,
#                color = "red", size = 0.4) +
xlab("") +
ylab("Broj članaka") +
#  annotation_custom(grob) +
scale_x_date(breaks = pretty_breaks(10))
br_cl_dom <- mima %>%
mutate(Datum = floor_date(DATE, "week")) %>%
group_by(Datum, FROM) %>%
count() %>%
arrange(desc(FROM)) %>%
ggplot(.,aes(Datum, n)) +
geom_line(size = 1.1) +
facet_wrap(~ FROM) +
theme_bw() +
#  geom_hline(yintercept = 40.5	,
#                color = "red", size = 0.4) +
xlab("") +
ylab("Broj članaka") +
#  annotation_custom(grob) +
scale_x_date(breaks = pretty_breaks(10))
br_cl_dom
vazni_domena <- rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
#  mutate(Datum = floor_date(datum, "week")) %>%
group_by(FROM) %>%
count(word) %>%
mutate(ukupno_n = sum(n)) %>%
arrange(desc(n)) %>% #View()
filter(word %in% c("Mima",
"Lesnina",
"Harvey Norman")) %>% #View()
ggplot(., aes(word, n / ukupno_n , fill = word  )) +
geom_bar(stat = "identity") +
facet_wrap(~ FROM, scales = "free_y")   +
scale_y_continuous(labels = scales::percent_format()) +
ggtitle("Zastupljenost \"najizglednijih\" kandidata na portalima") +
ylab("Udio %") +
xlab("") +
labs(caption = "*Udio se odnosi na učestalost pojavljivanja imena kandidata u ukupnom broju riječi na domeni")+
theme(axis.title.x=element_blank(),
axis.text.x=element_blank(),
axis.ticks.x=element_blank())
vazni_domena
vazni_kandidati <- rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
mutate(Datum = floor_date(DATE, "week")) %>%
group_by(Datum) %>%
count(word) %>%
mutate(ukupno_n = sum(n)) %>%
arrange(desc(n)) %>% #View()
#  filter(word %in% c("jysk",
#                     "lesnina",
#                     "harvey")) %>%
ggplot(., aes(Datum, n / ukupno_n)) +
geom_point() +
geom_smooth() +
facet_wrap(~ word, scales = "free_y") +
scale_y_continuous(labels = scales::percent_format()) +
ggtitle("Učestalost spominjanja riječi") +
ylab("Udio %") +
xlab("") +
#  scale_x_date(breaks = "week") %>%
labs(caption = "Udio se odnosi na učestalost pojavljivanja riječi u ukupnom broju riječi na tjednoj osnovi") +
scale_x_date(breaks = pretty_breaks(10))
rijeci_clean
rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
mutate(Datum = floor_date(DATE, "week"))
rijeci_clean
vazni_kandidati <- rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
#  mutate(Datum = floor_date(DATE, "week")) %>%
group_by(DATE) %>%
count(word) %>%
mutate(ukupno_n = sum(n)) %>%
arrange(desc(n)) %>% #View()
#  filter(word %in% c("jysk",
#                     "lesnina",
#                     "harvey")) %>%
ggplot(., aes(DATE, n / ukupno_n)) +
geom_point() +
geom_smooth() +
facet_wrap(~ word, scales = "free_y") +
scale_y_continuous(labels = scales::percent_format()) +
ggtitle("Učestalost spominjanja riječi") +
ylab("Udio %") +
xlab("") +
#  scale_x_date(breaks = "week") %>%
labs(caption = "Udio se odnosi na učestalost pojavljivanja riječi u ukupnom broju riječi na tjednoj osnovi") +
scale_x_date(breaks = pretty_breaks(10))
vazni_kandidati
rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
#  mutate(Datum = floor_date(DATE, "week")) %>%
group_by(DATE) %>%
count(word) %>%
mutate(ukupno_n = sum(n)) %>%
arrange(desc(n)) %>% #View()
#  filter(word %in% c("jysk",
#                     "lesnina",
#                     "harvey")) %>%
ggplot(., aes(DATE, n / ukupno_n)) +
geom_point() +
geom_smooth()
View(rijeci_clean)
rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
#  mutate(Datum = floor_date(DATE, "week")) %>%
group_by(DATE) %>%
count(word) %>%
mutate(ukupno_n = sum(n)) %>%
arrange(desc(n)) %>% #View()
#  filter(word %in% c("jysk",
#                     "lesnina",
#                     "harvey")) %>%
ggplot(., aes(Datum, n / ukupno_n)) +
geom_point() +
geom_smooth() +
facet_wrap(~ word, scales = "free_y")
rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
#  mutate(Datum = floor_date(DATE, "week")) %>%
group_by(DATE) %>%
count(word) %>%
mutate(ukupno_n = sum(n)) %>%
arrange(desc(n)) %>% #View()
#  filter(word %in% c("jysk",
#                     "lesnina",
#                     "harvey")) %>%
ggplot(., aes(DATE, n / ukupno_n)) +
geom_point() +
geom_smooth()
rijeci_clean %>%
#  filter(datum > "2019-06-02" & datum < "2019-09-15"	) %>%
#  mutate(Datum = floor_date(DATE, "week")) %>%
group_by(DATE) %>%
count(word) %>%
mutate(ukupno_n = sum(n)) %>%
arrange(desc(n)) %>% #View()
#  filter(word %in% c("jysk",
#                     "lesnina",
#                     "harvey")) %>%
ggplot(., aes(DATE, n / ukupno_n)) +
geom_point() +
geom_smooth() +
facet_wrap(~ word, scales = "free_y")
rm(list=ls())
library(estudy2)
library(plotly)
library(TSstudio)
library(jsonlite)
library(plyr)
library(zoo)
library(httr)
library(plotly)
library(jsonlite)
library(dplyr)
library(tidyr)
library(Quandl)
library(stringr)
library(eventstudies)
ISIN <- readxl::read_xlsx("./Data/ISIN_list.xlsx") %>% select(ISIN)
links <- c()
rawDta <- c()
for( i in ISIN){
links <- paste0('https://zse.hr/json/securityHistory/', i,
'/2019-01-01/2021-04-13/hr?trading_model_id=ALL')
}
for (i in links){
rawDta[[i]] <- fromJSON(content(GET(i), as = "text", encoding = "UTF-8"))
}
stockDF <- lapply(rawDta, '[[', "rows") %>% bind_rows()
source("./Secret/Passkey.R")
Crobex <- Quandl("ZAGREBSE/CROBEX", type = "zoo",collapse = "daily",start_date = "2019-01-02", end_date = Sys.Date())
gold <- Quandl("LBMA/GOLD", type = "zoo", collapse = "daily", start_date = "2019-01-01",end_date = Sys.Date())
gold <- gold[,5]
TOURISMdta <- stockDF %>% select(Date = date, Close = last_price, url) %>%
mutate(Date = gsub("[.]$","", Date)) %>%
mutate(Ticker = str_sub(.$url,-37,-34)) %>%
mutate(Close = as.numeric(gsub("(.*),.*", "\\1", Close))) %>%
select(-url) %>%
group_by(Ticker) %>%
mutate(n = n()) %>%
ungroup() %>%
filter(n > 100) %>%
select(-n) %>%
group_by(Ticker) %>%
distinct(Date, .keep_all=TRUE) %>%
ungroup() %>%
tidyr::spread(Ticker, Close, fill=0) %>%
mutate( Date = as.Date(Date,"%d.%m.%Y")) %>%
arrange(desc(Date))
TOURISMdta <- stockDF %>% select(Date = date, Close = last_price, url) %>%
mutate(Date = gsub("[.]$","", Date)) %>%
mutate(Ticker = str_sub(.$url,-37,-34)) %>%
mutate(Close = as.numeric(gsub("(.*),.*", "\\1", Close))) %>%
select(-url) %>%
group_by(Ticker) %>%
mutate(n = n()) %>%
ungroup() %>%
filter(n > 100) %>%
select(-n) %>%
group_by(Ticker) %>%
distinct(Date, .keep_all=TRUE) %>%
ungroup() %>%
tidyr::spread(Ticker, Close, fill=0) %>%
mutate( Date = as.Date(Date,"%d.%m.%Y")) %>%
arrange(desc(Date))
TOURISMdta <- stockDF %>% select(Date = date, Close = last_price, url) %>%
mutate(Date = gsub("[.]$","", Date)) %>%
mutate(Ticker = str_sub(.$url,-37,-34)) %>%
mutate(Close = as.numeric(gsub("(.*),.*", "\\1", Close))) %>%
select(-url) %>%
group_by(Ticker) %>%
dplyr::mutate(n = n()) %>%
ungroup() %>%
dplyr::filter(n > 100) %>%
dplyr::select(-n) %>%
group_by(Ticker) %>%
distinct(Date, .keep_all=TRUE) %>%
ungroup() %>%
tidyr::spread(Ticker, Close, fill=0) %>%
mutate( Date = as.Date(Date,"%d.%m.%Y")) %>%
arrange(desc(Date))
TOURISMdta <- zoo(TOURISMdta[,-1], order.by = TOURISMdta$Date)
rates <- get_rates_from_prices(TOURISMdta,
quote = "Close",
multi_day = TRUE,
compounding = "continuous")
rates
ratesDF <- data.frame(Date = rownames(rates), rates) %>% mutate(Date = as.Date(Date,"%Y-%m-%d"))
ratesDF <- data.frame(rates)
ratesDF <- data.frame(Date = rownames(rates), rates) %>% mutate(Date = as.Date(Date,"%Y-%m-%d"))
View(ratesDF)
ratesDF <- data.frame(Date = rownames(ratesDF), rates) %>% mutate(Date = as.Date(Date,"%Y-%m-%d"))
rownames(ratesDF) <- NULL
ratesDF[sapply(ratesDF, is.infinite)] <- NA
ratesDF[sapply(ratesDF, is.nan)] <- NA
ratesDF[sapply(ratesDF, is.na)] <- 0
rates <-  zoo(ratesDF[,-1], order.by = ratesDF$Date)
rates_indx <- get_rates_from_prices(Crobex,
quote = "Close",
multi_day = TRUE,
compounding = "continuous")
colnames(rates_indx) <- "Crobex"
securities_returns <- apply_market_model(
rates = rates,
regressor = rates_indx,
same_regressor_for_all = TRUE,
market_model = "sim",
estimation_method = "ols",
estimation_start = as.Date("2019-01-01"),
estimation_end = as.Date("2020-01-23")
)
kable(parametric_tests(list_of_returns = securities_returns,
event_start = as.Date("2020-02-24"),
event_end = as.Date("2020-04-14")))
